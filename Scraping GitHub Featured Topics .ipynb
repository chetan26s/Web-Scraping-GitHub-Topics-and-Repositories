{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Featured Topics Popular Repositories from GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Web Scraping \n",
    "\n",
    "- Web Scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications.\n",
    "\n",
    "### Why Perform Web Scraping \n",
    "- In data science, to do anything, you need to have data at hand. To get that data, you’ll need to research the required sources, and web scraping helps you. Web scraping collects and categorizes all the required data in one accessible location. Researching with a single, convenient location is much more feasible and more comfortable than searching for everything one-by-one.\n",
    "\n",
    "\n",
    "\n",
    "### What is GitHub \n",
    "- GitHub is an open-source repository hosting service, sort of like a cloud for code. It hosts your source code projects in a variety of different programming languages and keeps track of the various changes made to every iteration. URL : https://github.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__We’ll be using the packages__\n",
    "\n",
    "- Requests — for downloading the HTML code from the TMdb URL\n",
    "- BeautifulSoup4 — for extracting data from the HTML string\n",
    "- Pandas — to gather my data into a dataframe for further processingPandas and Python Programming Language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Outline\n",
    "\n",
    "\n",
    "Let's see an outline of the steps we'll follow:\n",
    "\n",
    "1. Load the GitHub https://github.com/ using Requests.\n",
    "2. Parse the HTML web page using BeautifulSoup.\n",
    "3. Extract the list of Topics from the landing page. For each topic, we'll get the topic name, topic description and URL of the topic. \n",
    "4. Again for each topic name, we'll grab the username, repository name, star counts and repository URL.\n",
    "5. Compile extracted repositories details into Python Lists and Dictionaries.\n",
    "6. We'll extend the above logic to scrape multiple repositories.z\n",
    "7. Finally, we'll save all the repository informations into a csv file.\n",
    "\n",
    "  \n",
    "  The csv file will be of the following format.\n",
    "\n",
    "     repo name,username,stars,repo_url\n",
    "     CS_notes,CyC2018,133000,https://github.com/CyC2018/CS-Notes\n",
    "     tech-interview-handbook,yangshun,111000,https://github.com/yangshun/tech-interview-handbook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the Libraries\n",
    "\n",
    "Let’s start by installing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandas\n",
    "!pip install pandas as pd --quiet\n",
    "\n",
    "# Install the bs4 module from BeautifulSoup \n",
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import necessary packages \n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Webpage using Requests\n",
    "\n",
    "The landing page of GitHub page consists of a list of featured topics. We can click on each of the topic and navigate to the individual topic page to get more details of each topic.\n",
    "\n",
    "Each topic page contains popular repositories. From the landing page, we will parse the list of repoitory names, usernames,  stars count and repository URLs. Then, we can navigate to the next pages using the ‘Load More’ button click."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the requests library to download web pages\n",
    "- Download and save web pages locally using the requests library.\n",
    "- Use BeatifulSoup to parse the website.\n",
    "- Inspect the website's HTML source and identify the right URLs to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub URL \n",
    "github_url = 'https://github.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The github page is downloaded using 'requests`\n",
    "response = requests.get(github_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the request was successful \n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code validates if the requests was successful using the .status_code = 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html lang=\"en\" >\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-VfI/oBxJKY4+ZNd24vTPiyumBP7aGW4fepRsD/fUTuz8Ebw8WLpgvNkCNjj8+YRFbtZUy'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_contents = response.text\n",
    "page_contents[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('github.html', 'w') as f:\n",
    "    f.write(page_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc = BeautifulSoup(page_contents, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The HTML page content is extracted using BeautifulSoup Library into `topic_doc`.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to perform the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page():\n",
    "    \"\"\"\n",
    "    Function to download a web page using 'requests' and check the status code to validate\n",
    "    if the call was successful. \n",
    "    \"\"\"\n",
    "    topic_url = 'https://github.com/topics'\n",
    "    \n",
    "    # Access the webpage using 'requests'\n",
    "    response = requests.get(topic_url)\n",
    "    # Check if request is successful.\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    # Parse using BeaurifulSoup.\n",
    "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc = get_topic_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(topic_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224145"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html lang=\"en\" >\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-VfI/oBxJKY4+ZNd24vTPiyumBP7aGW4fepRsD/fUTuz8Ebw8WLpgvNkCNjj8+YRFbtZUyBfJ9UTEzXippSqvaA==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/frameworks-55f23fa01c49298e3e64d776e2f4cf8b.css\" />\\n  \\n    <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-0G3kkMTKku66NTv2HdOjTCO1C5CgsLk4X7y+7ognYA7yZJRx+/7e/HF5Dzdp3R8mgHIPmKWkeQzO8c8drKzCjA==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/behaviors-d06de490c4ca92eeba353bf61dd3a34c.css\" />\\n    \\n    \\n    \\n    <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-iiK72fBJnLBFZ4aw'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_contents = response.text\n",
    "page_contents[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Web page\n",
    "\n",
    "Chrome users can use the “Inspect” option by right-clicking on the page to examine the HTML code behind the page. A menu will appear, either on the bottom or right side of the page (based on the settings), with a long list of nested HTML tags. To find the correct tag associated with the information needed, select the details (ex. topic name) and click “Inspect” again and that will highlight a blue box. Now, you can click on the HTML tags and get the correct tag associated with the item of interest, here, topic name.\n",
    "\n",
    "As we see in the image below, the 'topic names' are embedded in the 'p' tags with class ' f3 lh-condensed mb-0 mt-1 Link--primary '.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating some helper functions to parse the information from the page\n",
    "\n",
    "- To get topic titles, we can pick 'p' tags with the class 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "\n",
    "![](https://i.imgur.com/PDSwDE0.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the a.text.strip() to retrieve the name of the topic. Note, that we need to exclude the above lines, as those do not contain the topic names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_title_tags = topic_doc.find_all('p',{'class':'f3 lh-condensed mb-0 mt-1 Link--primary'})\n",
    "topic_titles = []\n",
    "for tag in topic_title_tags:\n",
    "    topic_titles.append(tag.text)\n",
    "topic_titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D modeling is the process of virtually developing the surface and structure of a 3D object.',\n",
       " 'Ajax is a technique for creating interactive web applications.',\n",
       " 'Algorithms are self-contained sequences that carry out a variety of tasks.',\n",
       " 'Amp is a non-blocking concurrency framework for PHP.',\n",
       " 'Android is an operating system built by Google designed for mobile devices.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_desc_tags = topic_doc.find_all('p',class_='f5 color-text-secondary mb-0 mt-1')\n",
    "topic_descs = []\n",
    "\n",
    "for tag in topic_desc_tags:\n",
    "    topic_descs.append(tag.text.strip())\n",
    "\n",
    "topic_descs[:5]   # top 5 topic description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/topics/3d',\n",
       " '/topics/ajax',\n",
       " '/topics/algorithm',\n",
       " '/topics/amphp',\n",
       " '/topics/android']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_link_tags = topic_doc.find_all('a',class_='d-flex no-underline')\n",
    "\n",
    "topic_urls = []\n",
    "\n",
    "for tag in topic_link_tags:\n",
    "    topic_urls.append(tag['href'])\n",
    "\n",
    "topic_urls[:5]   # top 5 topic URL's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create Function for Topic Titles, Topic Description and Topic URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(topic_doc):\n",
    "    \"\"\"\n",
    "    Function to extract the topic titles from HTML source code using BeautifulSoup.\n",
    "    \"\"\"\n",
    "    topic_title_tags = topic_doc.find_all('p',{'class':'f3 lh-condensed mb-0 mt-1 Link--primary'})\n",
    "    topic_titles = []\n",
    "    # Loop through the page get all the  topic titles from the page\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "        \n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'get topic titles ' can be used to get the list of titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the popular movie list from the webpage using the BeautifulSoup object `doc`. \n",
    "\n",
    "get_topic_titles(topic_doc)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the list of featured topic titles in the landing page of the GitHub. \n",
    "\n",
    "Similarly,  let's define functions for topic description and URLs.\n",
    "\n",
    "The description are embedded as part of the `p` tag under the `f5 color-text-secondary mb-0 mt-1` class in the webpage as below.\n",
    "\n",
    "![](https://imgur.com/PuXp524.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(topic_doc):\n",
    "    \"\"\"\n",
    "    Function to extract the topic description from HTML source code using the BeautifulSoup. \n",
    "    \"\"\"\n",
    "    topic_desc_tags = topic_doc.find_all('p',class_='f5 color-text-secondary mb-0 mt-1')\n",
    "    topic_descs = []\n",
    "    \n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "        \n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D modeling is the process of virtually developing the surface and structure of a 3D object.',\n",
       " 'Ajax is a technique for creating interactive web applications.',\n",
       " 'Algorithms are self-contained sequences that carry out a variety of tasks.',\n",
       " 'Amp is a non-blocking concurrency framework for PHP.',\n",
       " 'Android is an operating system built by Google designed for mobile devices.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the topic description of each topic in the webpage using the BeautifulSoup object `topic_doc`. \n",
    "\n",
    "get_topic_descs(topic_doc)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the top 5 topics in the landing page of the GitHub web page. \n",
    "\n",
    "Each topic URL can be retrieved by appending the base URL of https://www.github to .a['href'].\n",
    "\n",
    "![](https://imgur.com/yaDTGOI.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(topic_doc):\n",
    "    \"\"\"\n",
    "    Function to extract the topic URLs from HTML source code using the BeautifulSoup. \n",
    "    \"\"\"\n",
    "    topic_link_tags = topic_doc.find_all('a', {'class': 'd-flex no-underline'})\n",
    "    topic_urls = []\n",
    "    \n",
    "    base_url = 'https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/topics/3d',\n",
       " 'https://github.com/topics/ajax',\n",
       " 'https://github.com/topics/algorithm',\n",
       " 'https://github.com/topics/amphp',\n",
       " 'https://github.com/topics/android']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the topic URL of each topic in the webpage using the BeautifulSoup object `topic_doc`. \n",
    "\n",
    "get_topic_urls(topic_doc)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic():\n",
    "    topic_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "\n",
    "topics_dict = {\n",
    "        \n",
    "        'title' : get_topic_titles(topic_doc),\n",
    "        'description' : get_topic_descs(topic_doc),\n",
    "        'url' : get_topic_urls(topic_doc)\n",
    "    }\n",
    "    \n",
    "topics_df = pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>3D modeling is the process of virtually develo...</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>Algorithms are self-contained sequences that c...</td>\n",
       "      <td>https://github.com/topics/algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amp</td>\n",
       "      <td>Amp is a non-blocking concurrency framework fo...</td>\n",
       "      <td>https://github.com/topics/amphp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Android</td>\n",
       "      <td>Android is an operating system built by Google...</td>\n",
       "      <td>https://github.com/topics/android</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Angular</td>\n",
       "      <td>Angular is an open source web application plat...</td>\n",
       "      <td>https://github.com/topics/angular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ansible</td>\n",
       "      <td>Ansible is a simple and powerful automation en...</td>\n",
       "      <td>https://github.com/topics/ansible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>API</td>\n",
       "      <td>An API (Application Programming Interface) is ...</td>\n",
       "      <td>https://github.com/topics/api</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Arduino</td>\n",
       "      <td>Arduino is an open source hardware and softwar...</td>\n",
       "      <td>https://github.com/topics/arduino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASP.NET</td>\n",
       "      <td>ASP.NET is a web framework for building modern...</td>\n",
       "      <td>https://github.com/topics/aspnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Atom</td>\n",
       "      <td>Atom is a open source text editor built with w...</td>\n",
       "      <td>https://github.com/topics/atom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Awesome Lists</td>\n",
       "      <td>An awesome list is a list of awesome things cu...</td>\n",
       "      <td>https://github.com/topics/awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Amazon Web Services</td>\n",
       "      <td>Amazon Web Services provides on-demand cloud c...</td>\n",
       "      <td>https://github.com/topics/aws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Azure</td>\n",
       "      <td>Azure is a cloud computing service created by ...</td>\n",
       "      <td>https://github.com/topics/azure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Babel</td>\n",
       "      <td>Babel is a compiler for writing next generatio...</td>\n",
       "      <td>https://github.com/topics/babel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bash</td>\n",
       "      <td>Bash is a shell and command language interpret...</td>\n",
       "      <td>https://github.com/topics/bash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bitcoin</td>\n",
       "      <td>Bitcoin is a cryptocurrency developed by Satos...</td>\n",
       "      <td>https://github.com/topics/bitcoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bootstrap</td>\n",
       "      <td>Bootstrap is an HTML, CSS, and JavaScript fram...</td>\n",
       "      <td>https://github.com/topics/bootstrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bot</td>\n",
       "      <td>A bot is an application that runs automated ta...</td>\n",
       "      <td>https://github.com/topics/bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C</td>\n",
       "      <td>C is a general purpose programming language th...</td>\n",
       "      <td>https://github.com/topics/c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Chrome</td>\n",
       "      <td>Chrome is a web browser from the tech company ...</td>\n",
       "      <td>https://github.com/topics/chrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Chrome extension</td>\n",
       "      <td>Google Chrome Extensions are add-ons that allo...</td>\n",
       "      <td>https://github.com/topics/chrome-extension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Command line interface</td>\n",
       "      <td>A CLI, or command-line interface, is a console...</td>\n",
       "      <td>https://github.com/topics/cli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Clojure</td>\n",
       "      <td>Clojure is a dynamic, general-purpose programm...</td>\n",
       "      <td>https://github.com/topics/clojure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Code quality</td>\n",
       "      <td>Automate your code review with style, quality,...</td>\n",
       "      <td>https://github.com/topics/code-quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Code review</td>\n",
       "      <td>Ensure your code meets quality standards and s...</td>\n",
       "      <td>https://github.com/topics/code-review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Compiler</td>\n",
       "      <td>Compilers are software that translate higher-l...</td>\n",
       "      <td>https://github.com/topics/compiler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Continuous integration</td>\n",
       "      <td>Automatically build and test your code as you ...</td>\n",
       "      <td>https://github.com/topics/continuous-integration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>The coronavirus disease 2019 (COVID-19) is an ...</td>\n",
       "      <td>https://github.com/topics/covid-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>C++</td>\n",
       "      <td>C++ is a general purpose and object-oriented p...</td>\n",
       "      <td>https://github.com/topics/cpp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                        description  \\\n",
       "0                       3D  3D modeling is the process of virtually develo...   \n",
       "1                     Ajax  Ajax is a technique for creating interactive w...   \n",
       "2                Algorithm  Algorithms are self-contained sequences that c...   \n",
       "3                      Amp  Amp is a non-blocking concurrency framework fo...   \n",
       "4                  Android  Android is an operating system built by Google...   \n",
       "5                  Angular  Angular is an open source web application plat...   \n",
       "6                  Ansible  Ansible is a simple and powerful automation en...   \n",
       "7                      API  An API (Application Programming Interface) is ...   \n",
       "8                  Arduino  Arduino is an open source hardware and softwar...   \n",
       "9                  ASP.NET  ASP.NET is a web framework for building modern...   \n",
       "10                    Atom  Atom is a open source text editor built with w...   \n",
       "11           Awesome Lists  An awesome list is a list of awesome things cu...   \n",
       "12     Amazon Web Services  Amazon Web Services provides on-demand cloud c...   \n",
       "13                   Azure  Azure is a cloud computing service created by ...   \n",
       "14                   Babel  Babel is a compiler for writing next generatio...   \n",
       "15                    Bash  Bash is a shell and command language interpret...   \n",
       "16                 Bitcoin  Bitcoin is a cryptocurrency developed by Satos...   \n",
       "17               Bootstrap  Bootstrap is an HTML, CSS, and JavaScript fram...   \n",
       "18                     Bot  A bot is an application that runs automated ta...   \n",
       "19                       C  C is a general purpose programming language th...   \n",
       "20                  Chrome  Chrome is a web browser from the tech company ...   \n",
       "21        Chrome extension  Google Chrome Extensions are add-ons that allo...   \n",
       "22  Command line interface  A CLI, or command-line interface, is a console...   \n",
       "23                 Clojure  Clojure is a dynamic, general-purpose programm...   \n",
       "24            Code quality  Automate your code review with style, quality,...   \n",
       "25             Code review  Ensure your code meets quality standards and s...   \n",
       "26                Compiler  Compilers are software that translate higher-l...   \n",
       "27  Continuous integration  Automatically build and test your code as you ...   \n",
       "28                COVID-19  The coronavirus disease 2019 (COVID-19) is an ...   \n",
       "29                     C++  C++ is a general purpose and object-oriented p...   \n",
       "\n",
       "                                                 url  \n",
       "0                       https://github.com/topics/3d  \n",
       "1                     https://github.com/topics/ajax  \n",
       "2                https://github.com/topics/algorithm  \n",
       "3                    https://github.com/topics/amphp  \n",
       "4                  https://github.com/topics/android  \n",
       "5                  https://github.com/topics/angular  \n",
       "6                  https://github.com/topics/ansible  \n",
       "7                      https://github.com/topics/api  \n",
       "8                  https://github.com/topics/arduino  \n",
       "9                   https://github.com/topics/aspnet  \n",
       "10                    https://github.com/topics/atom  \n",
       "11                 https://github.com/topics/awesome  \n",
       "12                     https://github.com/topics/aws  \n",
       "13                   https://github.com/topics/azure  \n",
       "14                   https://github.com/topics/babel  \n",
       "15                    https://github.com/topics/bash  \n",
       "16                 https://github.com/topics/bitcoin  \n",
       "17               https://github.com/topics/bootstrap  \n",
       "18                     https://github.com/topics/bot  \n",
       "19                       https://github.com/topics/c  \n",
       "20                  https://github.com/topics/chrome  \n",
       "21        https://github.com/topics/chrome-extension  \n",
       "22                     https://github.com/topics/cli  \n",
       "23                 https://github.com/topics/clojure  \n",
       "24            https://github.com/topics/code-quality  \n",
       "25             https://github.com/topics/code-review  \n",
       "26                https://github.com/topics/compiler  \n",
       "27  https://github.com/topics/continuous-integration  \n",
       "28                https://github.com/topics/covid-19  \n",
       "29                     https://github.com/topics/cpp  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.to_csv('topics_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Saving the topics information into csv format.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Popular 30 Repositories from  Each Topic Page to Scrape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now we have topic titles, topic description and topic URLs for the first page.\n",
    "\n",
    "Let’s first consider a sample topic web page: Amazon Web Services and see how we parse HTML tags to get additional information like username, repository name, star count and top 25 repository URL's of each of the topics. \n",
    "![](https://imgur.com/0diXK8r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this all together into a single function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To read additional topic information, let's create a function that can accept a topic url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read a topic page\n",
    "def get_topic_page(topic_url):\n",
    "    \"\"\"\n",
    "    Function to read the HTML source code using BeautifulSoup.\n",
    "    \"\"\"\n",
    "    # download the page\n",
    "    response = requests.get(topic_url)\n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    # Parse using Beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc = get_topic_page('https://github.com/topics/aws')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the HTML source code in the BeautifulSoup object 'topic_doc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_selection_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
    "repo_tags = topic_doc.find_all('h1',{'class': h1_selection_class}) \n",
    "star_tags = topic_doc.find_all('a',class_='social-count float-none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serverless serverless https://github.com/serverless/serverless\n",
      "40.1k\n"
     ]
    }
   ],
   "source": [
    "a_tags = repo_tags[0].find_all('a')\n",
    "username = a_tags[0].text.strip()\n",
    "repo_name = a_tags[1].text.strip()\n",
    "\n",
    "\n",
    "base_url = 'https://github.com'\n",
    "repo_url = base_url + a_tags[1]['href']\n",
    "\n",
    "print(username,repo_name,repo_url)\n",
    "\n",
    "\n",
    "print(star_tags[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert 'k' to '1000' integer value\n",
    "\n",
    "def parse_star_count(stars_str):\n",
    "    stars_str = stars_str.strip()\n",
    "    if stars_str[-1] == 'k':\n",
    "        return int(float(stars_str[:-1]) * 1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40100"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_star_count(star_tags[0].text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the `h1` tag there are two `a` tags in first `a` tag contains the 'username' and in the second `a` tag contains 'repo_name'.\n",
    "\n",
    "In the `href`  under the `a` tag contins 'repo_url'\n",
    "\n",
    "In the `star tag` contains the 'stars count' for each repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(repo_tag,star_tag):\n",
    "    \"\"\"\n",
    "    Function to get the repository informations -\n",
    "    username , repo_name, repo_url and stars count\n",
    "    \"\"\"\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('serverless', 'serverless', 40100, 'https://github.com/serverless/serverless')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_repo_info(repo_tags[0],star_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’ve used Python Dictionary to store the key-value pairs of the topic information. Later, I've copied the dictionary to pandas DataFrame to store the tabular repository information into rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    \"\"\"\n",
    "    Function to get lists of repository information as lists from repo page. \n",
    "    \"\"\"\n",
    "    h1_selection_class = 'f3 color-text-secondary text-normal lh-condensed'\n",
    "    repo_tags = topic_doc.find_all('h1',{'class': h1_selection_class})\n",
    "    \n",
    "    star_tags = topic_doc.find_all('a',class_='social-count float-none')\n",
    "\n",
    "    topic_repos_dict = {\n",
    "        \n",
    "    'username':[],\n",
    "    'repo_name':[],\n",
    "    'stars':[],\n",
    "    'repo_url':[]\n",
    "        \n",
    "    }\n",
    "    # Loop through all the repositories.\n",
    "    \n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i],star_tags[i])\n",
    "        # get_repo_info returns username,repo_name,stars and repo_url.\n",
    "\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>stars</th>\n",
       "      <th>repo_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>serverless</td>\n",
       "      <td>serverless</td>\n",
       "      <td>40100</td>\n",
       "      <td>https://github.com/serverless/serverless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>serverless</td>\n",
       "      <td>serverless</td>\n",
       "      <td>31000</td>\n",
       "      <td>https://github.com/serverless/serverless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>serverless</td>\n",
       "      <td>serverless</td>\n",
       "      <td>21300</td>\n",
       "      <td>https://github.com/serverless/serverless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>serverless</td>\n",
       "      <td>serverless</td>\n",
       "      <td>11700</td>\n",
       "      <td>https://github.com/serverless/serverless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>serverless</td>\n",
       "      <td>serverless</td>\n",
       "      <td>11200</td>\n",
       "      <td>https://github.com/serverless/serverless</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     username   repo_name  stars                                  repo_url\n",
       "0  serverless  serverless  40100  https://github.com/serverless/serverless\n",
       "1  serverless  serverless  31000  https://github.com/serverless/serverless\n",
       "2  serverless  serverless  21300  https://github.com/serverless/serverless\n",
       "3  serverless  serverless  11700  https://github.com/serverless/serverless\n",
       "4  serverless  serverless  11200  https://github.com/serverless/serverless"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topic_repos(topic_doc)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the details that we are looking to retrieve from the repository web page username, repo_name, stars count and repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all Pieces Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have a funciton to get the list of topics\n",
    "- We have a function to create a CSV file for scraped repos from a topics page\n",
    "- Let's create a function to put them together\n",
    "- Let's run it scrape all the top repos for all the topics on the first page of GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get topic titles.\n",
    "def get_topic_titles(doc):\n",
    "    topic_title_tags = doc.find_all('p',{'class':'f3 lh-condensed mb-0 mt-1 Link--primary'})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "        \n",
    "    return topic_titles\n",
    "\n",
    "# function to get topic description.\n",
    "def get_topic_descs(doc):\n",
    "    topic_desc_tags = doc.find_all('p',class_='f5 color-text-secondary mb-0 mt-1')\n",
    "    topic_descs = []\n",
    "    \n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "        \n",
    "    return topic_descs\n",
    "\n",
    "# function to get topic urls.\n",
    "def topic_url(doc):\n",
    "    topic_link_tags = doc.find_all('a',class_='d-flex no-underline')\n",
    "    \n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "\n",
    "    return topic_urls\n",
    "\n",
    "\n",
    "# function to scrape topic using Beautiful Soup and storing them in tabular form.\n",
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    \n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    topics_dict = {\n",
    "        \n",
    "        'title' : get_topic_titles(doc),\n",
    "        'description' : get_topic_descs(doc),\n",
    "        'url' : topic_url(doc)\n",
    "    }\n",
    "    \n",
    "    # with the help of pandas storing them in tabular format.\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we have all the functions for topic titles, topic descriptions and topic url. \n",
    "Now from each topic URL we are extracting all the Top Repositories to get username, repo_name, stars count and repository URL.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_repos(topic_url,path):\n",
    "    \"\"\"\n",
    "    Function to scrape each topic and store them in the csv format.\n",
    "    If the file already exist skip scraping that topic and move to the next topic and so on.\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(\"The file {} already exists. Skipping...\".format(path))\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    #Each repo_info is saved in CSV file.\n",
    "    topic_df.to_csv(path ,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic_repos():\n",
    "    \"\"\"\n",
    "    Function to iterate over all the rows of repository url in the topics data frame.\n",
    "    \"\"\"\n",
    "    print('Scraping top topics from GitHub')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    #Create a folder 'data' to store all the repository inforamtion.\n",
    "    os.makedirs('data',exist_ok=True)\n",
    "    \n",
    "    #loop for each topic URL in the topic page.\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scrping top repositories for \"{}\"'.format(row['title']))\n",
    "        #then scrape_topic function runs through each URL to get repository information.\n",
    "        scrape_repos(row['url'], 'data/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping top topics from GitHub\n",
      "Scrping top repositories for \"3D\"\n",
      "The file data/3D.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Ajax\"\n",
      "The file data/Ajax.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Algorithm\"\n",
      "The file data/Algorithm.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Amp\"\n",
      "The file data/Amp.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Android\"\n",
      "The file data/Android.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Angular\"\n",
      "The file data/Angular.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Ansible\"\n",
      "The file data/Ansible.csv already exists. Skipping...\n",
      "Scrping top repositories for \"API\"\n",
      "The file data/API.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Arduino\"\n",
      "The file data/Arduino.csv already exists. Skipping...\n",
      "Scrping top repositories for \"ASP.NET\"\n",
      "The file data/ASP.NET.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Atom\"\n",
      "The file data/Atom.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Awesome Lists\"\n",
      "The file data/Awesome Lists.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Amazon Web Services\"\n",
      "The file data/Amazon Web Services.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Azure\"\n",
      "The file data/Azure.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Babel\"\n",
      "The file data/Babel.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Bash\"\n",
      "The file data/Bash.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Bitcoin\"\n",
      "The file data/Bitcoin.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Bootstrap\"\n",
      "The file data/Bootstrap.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Bot\"\n",
      "The file data/Bot.csv already exists. Skipping...\n",
      "Scrping top repositories for \"C\"\n",
      "The file data/C.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Chrome\"\n",
      "The file data/Chrome.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Chrome extension\"\n",
      "The file data/Chrome extension.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Command line interface\"\n",
      "The file data/Command line interface.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Clojure\"\n",
      "The file data/Clojure.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Code quality\"\n",
      "The file data/Code quality.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Code review\"\n",
      "The file data/Code review.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Compiler\"\n",
      "The file data/Compiler.csv already exists. Skipping...\n",
      "Scrping top repositories for \"Continuous integration\"\n",
      "The file data/Continuous integration.csv already exists. Skipping...\n",
      "Scrping top repositories for \"COVID-19\"\n",
      "The file data/COVID-19.csv already exists. Skipping...\n",
      "Scrping top repositories for \"C++\"\n",
      "The file data/C++.csv already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "scrape_topic_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__All the files are saved in CSV format in the `data` folder created__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In my project, I’m scraping 30 featured topics and since each page has many repositories I'm scraping the popular 30 repositories based on stars count.  It goes without saying that the more topics listing you want, the more web pages you should scrape.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1. Downloaded the GitHub web page using Requests.\n",
    "2. Extracted the topic details using BeautifulSoup (bs4).\n",
    "3. Extracted all the topic informations - topic title, topic description and topic urls.\n",
    "4. Complied the topic informations into Pandas lists and Dataframes.\n",
    "5. Extracted all the repository informations from each topic - username, repo_name, stars count and repo_urls.\n",
    "6. Complied the repository informations into Pandas lists and Dataframes.\n",
    "7. Extracted the repository informations for multiple topic pages.\n",
    "8. Saved all the dataset into .csv format in data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work and References \n",
    "\n",
    "In near future, I plan to\n",
    "\n",
    "- Perform web scraping using Selenium or Scrapy.\n",
    "- Perform Data Cleaning on the Data\n",
    "- Perform visualization on Data to get useful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
